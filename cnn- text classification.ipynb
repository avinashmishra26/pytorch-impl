{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "d556fd96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 255, 48])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 255, 24])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "m = nn.Conv1d(16, 255, 3, stride=1)\n",
    "input = torch.randn(1, 16, 50)\n",
    "output = m(input)\n",
    "print(output.shape)\n",
    "\n",
    "mp = nn.MaxPool1d(2)\n",
    "output = mp(output)\n",
    "output.shape\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b2012528",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class CharacterLevelCNN(nn.Module):\n",
    "    def __init__(self, args, number_of_classes):\n",
    "        super(CharacterLevelCNN, self).__init__()\n",
    "\n",
    "        # define conv layers\n",
    "\n",
    "        self.dropout_input = nn.Dropout2d(args.dropout_input)\n",
    "\n",
    "        self.conv1 = nn.Sequential(\n",
    "            nn.Conv1d(\n",
    "                args.number_of_characters + len(args.extra_characters),\n",
    "                256,\n",
    "                kernel_size=7,\n",
    "                padding=0,\n",
    "            ),\n",
    "            nn.ReLU(),\n",
    "            nn.MaxPool1d(3),\n",
    "        )\n",
    "\n",
    "        self.conv2 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=7, padding=0), nn.ReLU(), nn.MaxPool1d(3)\n",
    "        )\n",
    "\n",
    "        self.conv3 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv4 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv5 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU()\n",
    "        )\n",
    "\n",
    "        self.conv6 = nn.Sequential(\n",
    "            nn.Conv1d(256, 256, kernel_size=3, padding=0), nn.ReLU(), nn.MaxPool1d(3)\n",
    "        )\n",
    "\n",
    "        # compute the  output shape after forwarding an input to the conv layers\n",
    "\n",
    "        input_shape = (\n",
    "            128,\n",
    "            args.max_length,\n",
    "            args.number_of_characters + len(args.extra_characters),\n",
    "        )\n",
    "        self.output_dimension = self._get_conv_output(input_shape)\n",
    "\n",
    "        # define linear layers\n",
    "\n",
    "        self.fc1 = nn.Sequential(\n",
    "            nn.Linear(self.output_dimension, 1024), nn.ReLU(), nn.Dropout(0.5)\n",
    "        )\n",
    "\n",
    "        self.fc2 = nn.Sequential(nn.Linear(1024, 1024), nn.ReLU(), nn.Dropout(0.5))\n",
    "\n",
    "        self.fc3 = nn.Linear(1024, number_of_classes)\n",
    "\n",
    "        # initialize weights\n",
    "\n",
    "        self._create_weights()\n",
    "\n",
    "    # utility private functions\n",
    "\n",
    "    def _create_weights(self, mean=0.0, std=0.05):\n",
    "        for module in self.modules():\n",
    "            if isinstance(module, nn.Conv1d) or isinstance(module, nn.Linear):\n",
    "                module.weight.data.normal_(mean, std)\n",
    "\n",
    "    def _get_conv_output(self, shape):\n",
    "        x = torch.rand(shape)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        output_dimension = x.size(1)\n",
    "        return output_dimension\n",
    "\n",
    "    # forward\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.dropout_input(x)\n",
    "        x = x.transpose(1, 2)\n",
    "        x = self.conv1(x)\n",
    "        x = self.conv2(x)\n",
    "        x = self.conv3(x)\n",
    "        x = self.conv4(x)\n",
    "        x = self.conv5(x)\n",
    "        x = self.conv6(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b79f6bee",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import json\n",
    "import re\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# text-preprocessing\n",
    "\n",
    "\n",
    "def lower(text):\n",
    "    return text.lower()\n",
    "\n",
    "\n",
    "def remove_hashtags(text):\n",
    "    clean_text = re.sub(r\"#[A-Za-z0-9_]+\", \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_user_mentions(text):\n",
    "    clean_text = re.sub(r\"@[A-Za-z0-9_]+\", \"\", text)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "def remove_urls(text):\n",
    "    clean_text = re.sub(r\"^https?:\\/\\/.*[\\r\\n]*\", \"\", text, flags=re.MULTILINE)\n",
    "    return clean_text\n",
    "\n",
    "\n",
    "preprocessing_setps = {\n",
    "    \"remove_hashtags\": remove_hashtags,\n",
    "    \"remove_urls\": remove_urls,\n",
    "    \"remove_user_mentions\": remove_user_mentions,\n",
    "    \"lower\": lower,\n",
    "}\n",
    "\n",
    "\n",
    "def process_text(steps, text):\n",
    "    if steps is not None:\n",
    "        for step in steps:\n",
    "            text = preprocessing_setps[step](text)\n",
    "    return text\n",
    "\n",
    "\n",
    "# metrics // model evaluations\n",
    "\n",
    "\n",
    "def get_evaluation(y_true, y_prob, list_metrics):\n",
    "    y_pred = np.argmax(y_prob, -1)\n",
    "    output = {}\n",
    "    if \"accuracy\" in list_metrics:\n",
    "        output[\"accuracy\"] = metrics.accuracy_score(y_true, y_pred)\n",
    "    if \"f1\" in list_metrics:\n",
    "        output[\"f1\"] = metrics.f1_score(y_true, y_pred, average=\"weighted\")\n",
    "\n",
    "    return output\n",
    "\n",
    "\n",
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "\n",
    "# preprocess input for prediction\n",
    "\n",
    "\n",
    "def preprocess_input(args):\n",
    "    raw_text = args.text\n",
    "    steps = args.steps\n",
    "    for step in steps:\n",
    "        raw_text = preprocessing_setps[step](raw_text)\n",
    "\n",
    "    number_of_characters = args.number_of_characters + len(args.extra_characters)\n",
    "    identity_mat = np.identity(number_of_characters)\n",
    "    vocabulary = list(args.alphabet) + list(args.extra_characters)\n",
    "    max_length = args.max_length\n",
    "\n",
    "    processed_output = np.array(\n",
    "        [\n",
    "            identity_mat[vocabulary.index(i)]\n",
    "            for i in list(raw_text[::-1])\n",
    "            if i in vocabulary\n",
    "        ],\n",
    "        dtype=np.float32,\n",
    "    )\n",
    "    if len(processed_output) > max_length:\n",
    "        processed_output = processed_output[:max_length]\n",
    "    elif 0 < len(processed_output) < max_length:\n",
    "        processed_output = np.concatenate(\n",
    "            (\n",
    "                processed_output,\n",
    "                np.zeros(\n",
    "                    (max_length - len(processed_output), number_of_characters),\n",
    "                    dtype=np.float32,\n",
    "                ),\n",
    "            )\n",
    "        )\n",
    "    elif len(processed_output) == 0:\n",
    "        processed_output = np.zeros(\n",
    "            (max_length, number_of_characters), dtype=np.float32\n",
    "        )\n",
    "    return processed_output\n",
    "\n",
    "\n",
    "# cyclic learning rate scheduling\n",
    "\n",
    "\n",
    "def cyclical_lr(stepsize, min_lr=1.7e-3, max_lr=1e-2):\n",
    "\n",
    "    # Scaler: we can adapt this if we do not want the triangular CLR\n",
    "    def scaler(x):\n",
    "        return 1.0\n",
    "\n",
    "    # Lambda function to calculate the LR\n",
    "    def lr_lambda(it):\n",
    "        return min_lr + (max_lr - min_lr) * relative(it, stepsize)\n",
    "\n",
    "    # Additional function to see where on the cycle we are\n",
    "    def relative(it, stepsize):\n",
    "        cycle = math.floor(1 + it / (2 * stepsize))\n",
    "        x = abs(it / stepsize - 2 * cycle + 1)\n",
    "        return max(0, (1 - x)) * scaler(cycle)\n",
    "\n",
    "    return lr_lambda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "73899d5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "from collections import Counter\n",
    "\n",
    "from torch.utils.data import Dataset\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "def get_sample_weights(labels):\n",
    "    counter = Counter(labels)\n",
    "    counter = dict(counter)\n",
    "    for k in counter:\n",
    "        counter[k] = 1 / counter[k]\n",
    "    sample_weights = np.array([counter[l] for l in labels])\n",
    "    return sample_weights\n",
    "\n",
    "\n",
    "def load_data(args):\n",
    "    # chunk your dataframes in small portions\n",
    "    chunks = pd.read_csv(\n",
    "        args.data_path,\n",
    "        usecols=[args.text_column, args.label_column],\n",
    "        chunksize=args.chunksize,\n",
    "        encoding=args.encoding,\n",
    "        nrows=args.max_rows,\n",
    "        sep=args.sep,\n",
    "    )\n",
    "    texts = []\n",
    "    labels = []\n",
    "    for df_chunk in tqdm(chunks):\n",
    "        aux_df = df_chunk.copy()\n",
    "        aux_df = aux_df.sample(frac=1)\n",
    "        aux_df = aux_df[~aux_df[args.text_column].isnull()]\n",
    "        aux_df = aux_df[(aux_df[args.text_column].map(len) > 1)]\n",
    "        aux_df[\"processed_text\"] = aux_df[args.text_column].map(\n",
    "            lambda text: utils.process_text(args.steps, text)\n",
    "        )\n",
    "        texts += aux_df[\"processed_text\"].tolist()\n",
    "        labels += aux_df[args.label_column].tolist()\n",
    "\n",
    "    if bool(args.group_labels):\n",
    "\n",
    "        if bool(args.ignore_center):\n",
    "\n",
    "            label_ignored = args.label_ignored\n",
    "\n",
    "            clean_data = [\n",
    "                (text, label)\n",
    "                for (text, label) in zip(texts, labels)\n",
    "                if label not in [label_ignored]\n",
    "            ]\n",
    "\n",
    "            texts = [text for (text, label) in clean_data]\n",
    "            labels = [label for (text, label) in clean_data]\n",
    "\n",
    "            labels = list(map(lambda l: {1: 0, 2: 0, 4: 1, 5: 1}[l], labels))\n",
    "\n",
    "        else:\n",
    "            labels = list(map(lambda l: {1: 0, 2: 0, 3: 1, 4: 2, 5: 2}[l], labels))\n",
    "\n",
    "    if bool(args.balance):\n",
    "\n",
    "        counter = Counter(labels)\n",
    "        keys = list(counter.keys())\n",
    "        values = list(counter.values())\n",
    "        count_minority = np.min(values)\n",
    "\n",
    "        balanced_labels = []\n",
    "        balanced_texts = []\n",
    "\n",
    "        for key in keys:\n",
    "            balanced_texts += [\n",
    "                text for text, label in zip(texts, labels) if label == key\n",
    "            ][: int(args.ratio * count_minority)]\n",
    "            balanced_labels += [\n",
    "                label for text, label in zip(texts, labels) if label == key\n",
    "            ][: int(args.ratio * count_minority)]\n",
    "\n",
    "        texts = balanced_texts\n",
    "        labels = balanced_labels\n",
    "\n",
    "    number_of_classes = len(set(labels))\n",
    "\n",
    "    print(\n",
    "        f\"data loaded successfully with {len(texts)} rows and {number_of_classes} labels\"\n",
    "    )\n",
    "    print(\"Distribution of the classes\", Counter(labels))\n",
    "\n",
    "    sample_weights = get_sample_weights(labels)\n",
    "\n",
    "    return texts, labels, number_of_classes, sample_weights\n",
    "\n",
    "\n",
    "class MyDataset(Dataset):\n",
    "    def __init__(self, texts, labels, args):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.length = len(self.texts)\n",
    "\n",
    "        self.vocabulary = args.alphabet + args.extra_characters\n",
    "        self.number_of_characters = args.number_of_characters + len(\n",
    "            args.extra_characters\n",
    "        )\n",
    "        self.max_length = args.max_length\n",
    "        self.preprocessing_steps = args.steps\n",
    "        self.identity_mat = np.identity(self.number_of_characters)\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.length\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        raw_text = self.texts[index]\n",
    "\n",
    "        data = np.array(\n",
    "            [\n",
    "                self.identity_mat[self.vocabulary.index(i)]\n",
    "                for i in list(raw_text)[::-1]\n",
    "                if i in self.vocabulary\n",
    "            ],\n",
    "            dtype=np.float32,\n",
    "        )\n",
    "        if len(data) > self.max_length:\n",
    "            data = data[: self.max_length]\n",
    "        elif 0 < len(data) < self.max_length:\n",
    "            data = np.concatenate(\n",
    "                (\n",
    "                    data,\n",
    "                    np.zeros(\n",
    "                        (self.max_length - len(data), self.number_of_characters),\n",
    "                        dtype=np.float32,\n",
    "                    ),\n",
    "                )\n",
    "            )\n",
    "        elif len(data) == 0:\n",
    "            data = np.zeros(\n",
    "                (self.max_length, self.number_of_characters), dtype=np.float32\n",
    "            )\n",
    "\n",
    "        label = self.labels[index]\n",
    "        data = torch.Tensor(data)\n",
    "\n",
    "        return data, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "22fed466",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class FocalLoss(nn.Module):\n",
    "    def __init__(self, gamma=0, alpha=None, size_average=True):\n",
    "        super(FocalLoss, self).__init__()\n",
    "        self.gamma = gamma\n",
    "        self.alpha = alpha\n",
    "        if isinstance(alpha, (float, int)):\n",
    "            self.alpha = torch.Tensor([alpha, 1 - alpha])\n",
    "        if isinstance(alpha, list):\n",
    "            self.alpha = torch.Tensor(alpha)\n",
    "        self.size_average = size_average\n",
    "\n",
    "    def forward(self, input, target):\n",
    "        if input.dim() > 2:\n",
    "            # N,C,H,W => N,C,H*W\n",
    "            input = input.view(input.size(0), input.size(1), -1)\n",
    "            input = input.transpose(1, 2)  # N,C,H*W => N,H*W,C\n",
    "            input = input.contiguous().view(-1, input.size(2))  # N,H*W,C => N*H*W,C\n",
    "        target = target.view(-1, 1)\n",
    "\n",
    "        logpt = F.log_softmax(input, dim=1)\n",
    "        logpt = logpt.gather(1, target)\n",
    "        logpt = logpt.view(-1)\n",
    "        pt = Variable(logpt.data.exp())\n",
    "\n",
    "        if self.alpha is not None:\n",
    "            if self.alpha.type() != input.data.type():\n",
    "                self.alpha = self.alpha.type_as(input.data)\n",
    "            at = self.alpha.gather(0, target.data.view(-1))\n",
    "            logpt = logpt * Variable(at)\n",
    "\n",
    "        loss = -1 * (1 - pt) ** self.gamma * logpt\n",
    "        if self.size_average:\n",
    "            return loss.mean()\n",
    "        else:\n",
    "            return loss.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b1201cf",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pytorch",
   "language": "python",
   "name": "pytorch"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
